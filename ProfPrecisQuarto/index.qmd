---
title: "ProfPrecisAudreyBlarr"
---

My name is Audrey Blarr, and welcome to my Quarto-generated website!
Here, I will be posting my Professional Precis for CMPSC-203 Software Engineering.

This site features summaries, reflections, and use-cases regarding chapters of three books:
FuzzingBook, DebuggingBook, and Software Engineering at Google

To learn more about Quarto websites visit <https://quarto.org/docs/websites>.

# Software Engineering at Google

## Preface

#### Summary

The terms "software engineering" and "programming" have been used interchangeably over the years, although they have different implications. 
As software engineering isn't currently as rigorous as other areas of engineering, more tedious methods of engineering will be necessary as 
software becomes more commonly integrated into our lives. Software engineering can be referred to as "programming integrated over time", whereas 
we focus on not only writing the code itself, but the tools and processes needed to achieve and maintain sustainable code. Within these
processes, three main topics are of importance: "time and change", "scale and growth", and "trade-offs and costs".

#### Reflection

#### Use Cases

## What is Software Engineering?

#### Summary

Software engineers must concern themselves with the need for change as time passes, unlike typical engineers. Furthermore, there exists issues
of scale and efficiency, and outcomes with high stakes after making complex software decisions. Distinctions between "software engineering" and 
"programming" are very important, as the life span of given code allows for a prediction of its sustainability, or how much change will have to 
be implemented to the program's dependencies. Capability is important when deciding whether or not to implement change. You also must consider
the scale of the project, especially since software engineering is typically a team effort. Although team projects can pose new problems, there's
more potential in discovering viable solutions. Trade-offs must be evaluated often, as some implementations come with higher stakes than others.
The level of broader change necessary to a program is dependent on both the amount of programmers working on a project, and their levels of skill 
in programming. Projects must also be adjusted to changing externalities, such as upgrades to programs, depending on the value and necessity of the
given upgrade. Factors such as expertise, stability, conformity, familiarity, and policy can affect a codebase's flexibility. Some costs involved 
within trade-offs include financial, resource, personnel, transaction, opportunity, and societal costs. It's important to be evidence-driven rather
than making assumptions when debating trade-offs and the risks associated within.

#### Reflection

#### Use Cases

## How to Work Well on Teams

#### Summary

Cultural and social aspects of software engineering are crucial, as it's a team endeavor. A huge issue in the software engineering world includes 
insecurity, as people are afraid of others viewing their unfinished work or criticizing their completed work. A phenomenon known as The Genius Myth 
is exhibited by many in the software engineering industry as we idolize individuals such as Linus Torvalds and attest them as the single-handed creators
of large programs such as Linux. Although sparking the invention himself, Linus shared his implementations and discoveries with several others, allowing
for collaboration and new perspectives on debugging. The program is now a hundred times larger than its original kernal, and is being worked on by thousands. 
The Genius Myth emerges because we strive to imitate these big-name creators rather than ulitizing the resources of collaboration in fear of being critcized 
for not being a 'genius'. Hiding your code is super harmful, as you may not know if you're on the right track. It's easy to make fundamental mistakes early on,
and collaborating allows you to increase the Bus Factor of your code. If you, the sole creator of a program, were to hypothetically get hit by a bus, there would
be nothing stopping your project from failing to be implemented. Working with others is also much faster, and it's much better to be a component of a successful 
program as opposed to the crucial part of a failed one. Working in a team allows for an instantaneous feedback loop so you know when to correct your plans or 
designs. The 'three pillars' of social skills and success include 'humility', 'respect', and 'trust', and creating relationships through these skills allow for richer 
relationships with coworkers and an increased ability to complete the project. Properly documenting any failures within a project can prevent history from repeating 
itself and boost the understandings of others.

#### Reflection

#### Use Cases

## Knowledge Sharing

#### Summary

Many challenges can occur when a weak culture of learning exists, such as a lack of psychological safety (a fear of criticism or making mistakes) and information islands (lack of communication/shared resources), including information fragmentation, duplication, and skewing. Additionally, there are risks of single point of failure (related to the bus factor), all-or-nothing expertise (lack of middle-ground contributors), parroting (mindless copying), and haunted graveyards (avoiding areas of code out of fear). Software engineering is a multiperson process that relies on people and expertise. Personalized advice from experts is crucial, but it's limited to small numbers. Tribal knowledge, which exists between the knowledge of individual team members and what is documented, is also valuable. Experts can synthesize their knowledge and assess the relevance of documentation for individual use cases. Communication and coordination are essential for effective learning; no single knowledge-sharing approach is the correct solution for all learning styles, and the best approach will depend on the organization's growth. To learn efficiently you must first acknowledge your own lack of understanding, and welcome criticism rather than fear it. Mentorships can be extremely useful resources for novices who may lack either the understanding or confidence to efficiently contribute. Building this psychological safety beginning on a one-on-one scale can improve a novice's fear of contributing to larger-scale group solutions, and creating a safe environment without adversial behaviors/reactions is crucial. Learning is an ongoing process, so it's important to always ask questions or for further clarification when necessary, even if you consider yourself an expert. You also must ensure your understanding with pre-existing code and designs so you can properly decide if certain changes need to be implemented, rather than taking an easy route and rewriting the entire code. One-on-one help is limited in scale, so it's beneficial to seek help from the greater community. Through mediums such as group chats and mailing lists, you can not only participate in community-based learning, but also document the questions and answers discussed for other newcomers to rely on. Teaching is not just limited to experts; through office hours, tech talks, classes, and documenting/reviewing code, google engineers are constantly teaching each other their areas of expertise.  A culture that promotes open and honest knowledge sharing through common respect and recognition distributes that knowledge efficiently across the organization and allows that organization to scale over time.

#### Reflection

#### Use Cases




# FuzzingBook

## Introduction to Software Testing

#### Summary

It's essential to create testing for your implemented software to ensure accuracy. Simple examples of software testing 
may include calculations like square-root testing, where you can incorporate Python control structures (while, if), assignments,
and comparisons. You can incorporate 'print' statements into testing to observe closely how the function is being implemented. For 
instances like the square root example, you can test its accuracy by working backwards and multiplying the answer to itself.
However, it's more efficient to use automated testing, where you log the accurate answer and test the function to see if the 
output matches the desired answer. The 'assert' keyword can be used, which returns nothing if the function's output matches and raises an error if not.
You can use an 'epsilon' value to ensure your function's output remains below a specified value, which gets rid of any rounding inaccuracies. These tests 
can be incorporated straight into the implementation as well to save space and time. However, you must be able to both formulate and afford these run-time
checks. You can only test a finite amount of inputs, so there's a limit to the testing in that regard.

#### Reflection

#### Use Cases

## Coverage

#### Summary

The term "code coverage" refers to the ability to measure which parts of a program are being executed during a given test run, a process crucial for 
attempting to cover as much code as possible using a test generator. A simple Python command, 'cgi_decode()', will allow you to take an encoded string 
and decode it to its original form. You can derive tests using either Black-box testing, testing for specification, or White-box testing, testing for
implementation. White-box testing covers the structural features of a code, such as statement coverage and branch coverage. Furthermore, this method of 
testing keeps track of which code was executed so a programmer can then focus on writing tests to cover the uncovered code. Dynamic analysis can take 
place using the command 'sys.settrace()'to track how execution proceeds while decoding. Defining a 'Coverage' object to not only keep track of which 
lines are executed, but the function names as well, which is helpful when a program spans over many files. Along with Python, most other coding languages
have their own methods of testing code coverage. We must also set up a results checker to ensure that 'cgi_decode()' is error-free.

#### Reflection

#### Use Cases

## Fuzzing

#### Summary

The input of a program can take many forms, including data prompted by user or read from a file. These inputs determine how a program behaves, including its 
failures, so it's important to test each valid input, or a program's 'language'. Grammars are used to specify input languages formally, where a wide range of 
the properties of an input language is expressed, as well as the syntactical structure of an input. Consisting of a 'start' symbol followed by a set of 
expansion rules, grammers formally define the language in the sense that anything that can be derived from 'start' using the expansion rules is part of the 
language. Grammers can even be recursive and cover full arithmetic expressions. Using a format based on strings and lists is the simplest way to write grammars 
using Python. However, this method may be inefficient with its large number of search and replace operations, and it may not even produce a string. Grammars are 
not limited to just formal languages; they can be used to produce natural language as well. Grammar-generated inputs can be used as seeds in mutation-based 
fuzzing, where more complex constraints can be satisfied. Helper functions can be utilized in testing grammars, checking whether all used symbols are defined or 
not, and whether or not they're reachable from the starting point.

#### Reflection

#### Use Cases

## Mutation Analysis

#### Summary

Mutations, or artificial faults, can be injected into code to check whether or not a test suite will pick up on specific bugs. On subject programs there are two highlighted methods of running mutation analysis; MuFunctionAnalyzer (implementing mutation analysis for individual functions) and MuProgramAnalyzer (implementing mutation analysis for standalone programs containing test suites). Structural coverage, as discussed previously, may not be enough since an execution that produces a wrong output yet is unnoticed by the test suite is counted exactly the same as an execution that produces the right output. Ineffective tests may achieve 100% code coverage, yet inaccurately represent the severe lack of ability of the program to detect bugs. Seeding artificial faults, or mutations, into code and assessing the accuracy of the test suites helps to assess how the test suite would behave with real bugs. The set of valid tokens different from the original that make it past the compilation stage is considered to be its possible set of mutations that represent the probable faults in the program. The mutation score obtained represents the ability of any program analysis tools to prevent faults, and can then be used to judge static test suites, test generators, and execution frameworks. The test suite is supposed to only allow the original through, so any mutant that is not detected as faulty represents a bug in the test suite. Coverage is unable to determine the quality of assertion statements, which are an extremely important factor of test suite effectiveness, so injecting artificial faults allows us to better evaluate the quality of such assertions. Fault injection techniques analyze how the frequency of detection will provide us with the actual likelihood of the test suite to uncover bugs, provided we have a list of possible faults.  However, since generating these faults is a manual process, they will be biased by the preconceptions of the developer. The majority of faults in a program is likely due to small variations in a program's structure compared to the correct program. For a majority of larger faults composed of multiple smaller faults, a test suite detecting a single smaller fault is very likely to detect the larger fault that contains it. Generating a list of mutants, all possible valid variants of a program differing from the correct implementation by a smaller fault, is essential for generating test suites that apply to and kill each of these variants. For Python programs, we can convert the program into a tree (AST representation), then change small parts of the tree, which can then be passed through the Python interpreter for further processing. A simple way to produce a valid mutated version of a program is through replacing some of its statements with 'pass'. While MuFunctionAnalyzer utilizes mutations of specific functions, MuProgramAnalyzer is the main class responsible for mutation analysis of the test suite, accepting the name of the module to be tested and its source code. The method 'getitem' accepts the test module, fixes the import entries on the test module to correctly point to the mutant module, and passes it to the test runner 'MutantTestRunner'. The problem lies with equivalent mutants, mutants indistinguishable from the original in terms of semantics, whereas it becomes very difficult to judge the mutation score in their presence. A solution to this is inspecting mutants manually if they're small enough, or randomly selecting a smaller amount of mutants to manually inspect if there's a larger scale of them. Chao's estimator can also be utilized to compute the result of the complete test matrix of each test against each mutant.

#### Reflection

#### Use Cases
